defaults:
  - override hydra/launcher: submitit_local

exp_name: something_default
openai_key: null
openai_org: null
max_seq_tokens: 200 # Max token length of both text obs and goals

# Environment
env_spec:
  name: CrafterTextEnv-v1
  lm_spec:
    lm_class: SimpleOracle
    lm: code-davinci-002  # Only used for the GPT lm_classes (GPTLanguageModel, GPTHousekeep)
    prompt: BulletPrompt  # Format of the prompt
    max_tokens: ${max_seq_tokens}
    temperature: .1
    budget: 1 # In dollars
    openai_key: ${openai_key}
    openai_org: ${openai_key}
    all_goals: null # Automatically set to all goals in env for the novel-goals baseline
    max_num_goals: all # for baseline, suggest all possible goals or choose a number
    dummy_lm: False  # Use this if you want to test modifications to the LM without using OpenAI credits
    prob_threshold: .5  # For closed generation, probability of "yes" above which a goal is suggested.
    novelty_bonus: True  # Only reward for each goal once
  seed: ${seed}
  action_space_type: harder # set to 'easier' for smaller ac space env
  env_reward: ???   # to be specified later
  dying: True  # Can the agent die?
  length: 400  # Max episode length
  max_seq_len: ${max_seq_tokens}
  use_sbert: False  # Tokenize obs and goals with sbert
  device: "cuda"
  housekeep_task: rs_int  # Only used when env_spec.name is Housekeep
  housekeep_ep_num: 0  # Only used when env_spec.name is Housekeep
  use_language_state: False  # Agent is given encoded language state
  threshold: .99  # For open generation, cosine similarity threshold above which a goal is considered achieved
  single_task: null  # If not null, train on a single task, e.g. "chop tree"
  single_goal_hierarchical: False  # Train on a single goal, but suggest necessary subgoals first
  use_state_captioner: True  # Use learned captioner
  use_transition_captioner: True  # Use learned captioner
  check_ac_success: True  # Whether to check if an action is successful before rewarding for it
eval_video_goals: [] # Which goals to save videos of (slow to generate all)

# task settings
debug: False
frame_stack: 4
discount: 0.99
# train settings
num_train_frames: 5000000
num_seed_frames: 5000
# eval
eval_every_frames: 25000
num_eval_episodes: 10
single_goal_eval: false
# snapshot
save_snapshot: true
save_buffer: true
snapshot_path: null
resume_id: null
finetune_snapshot: False  # Load a saved agent but not the replay buffer
# replay buffer
replay_buffer_size: ${num_train_frames}
replay_buffer_num_workers: 4
offline_buffer: null
save_offline: False
use_offline_buffer: false
clip_reward: true
nstep: 3
batch_size: 64
# misc
seed: 1
device: cpu
save_video: False
use_tb: false
use_wandb: false
# experiment
experiment: exp
# agent
lr: 0.0000625
use_image: true
use_language_state: null # None for no language state, 'conv' for conv, 'transformer' for transformer, 'sbert' for sbert transformer
use_goal: false  # Show goal to agent
vocab_size: 102  # Only used when NOT using a pretrained encoder
train_log_every: 5000
finetune_settings: train_all  # Other options 'linear' or 'critic' (to only finetune those model components)
expl_agent_path: null  # If provided, uses this pretrained agent to guide exploration
other_model_prob: .5  # If using a pretrained agent to guide exploration, what fraction of the time to use it (rather than a random action)
train_after_reward: false  # Only start doing model updates after the agent has received its first positive reward (consider using for very sparse-rew envs)
decay_after_reward: false  # Only start decaying epsilon after the agent has received its first positive reward (consider using for very sparse-rew envs)

agent:
  _target_: dqn.DQNAgent
  obs_shape: ??? # to be specified later
  num_actions: ??? # to be specified later
  device: ${device}
  lr: ${lr}
  critic_target_tau: 1.0
  critic_target_update_every_steps: 8000
  train_eps_min: 0.01
  train_eps_max: 1
  train_eps_decay_steps: 1000000
  eval_eps: 0.001
  reward_scale: 1  # Scale reward (only use this if you want to try to balance pretraining and finetuning rewards better)
  update_every_steps: 4
  use_tb: ${use_tb}
  use_wandb: ${use_wandb}
  hidden_dim: 512
  use_image: ${use_image}
  use_language_state: ${use_language_state} # use transformer + language embeddings for states
  use_goal: ${use_goal}
  vocab: ${vocab_size}
  debug: ${debug}
  goal_encoder_type: sbert # gru or transformer or sbert
  # use_language: True # Uncomment or +args for RND/NovelD/APT
  # noveld: False # Uncomment or + args for RND/NovelD
  finetune_settings: ${finetune_settings}
  other_model_prob: ${other_model_prob}

use_extr_rew: False #Set to true if combining RND/NovelD/APT with extrinsic rewards

hydra:
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${exp_name}_${now:%H%M%S}
  sweep:
    dir: ./exp_local/${now:%Y.%m.%d}/${exp_name}_${now:%H%M%S}_seed${seed}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 32
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: ./exp_local/${now:%Y.%m.%d}/${exp_name}_${now:%H%M%S}/.slurm
